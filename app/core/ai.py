import os
import json
import re
import json5
from openai import AsyncOpenAI
from dotenv import load_dotenv
from typing import Optional, List, Dict, Any

load_dotenv()


EMOTION_LIST = [
    "neutral",
    "tense",
    "calm",
    "mysterious",
    "action",
    "happy",
    "sad",
    "angry",
    "surprised",
    "fearful",
    "excited",
    "bored",
    "curious",
    "confused",
    "annoyed",
    "satisfied",
    "disappointed"
]

def parse_json_with_fallback(content: str) -> Dict[str, Any]:
    """ä½¿ç”¨ json5 ä¼˜å…ˆè§£æ JSONï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ ‡å‡† json
    
    json5 æ”¯æŒæ›´å®½æ¾çš„ JSON æ ¼å¼ï¼š
    - å…è®¸å°¾éšé€—å·
    - å…è®¸å•å¼•å·å­—ç¬¦ä¸²
    - å…è®¸æœªè½¬ä¹‰çš„æ¢è¡Œç¬¦ï¼ˆåœ¨å­—ç¬¦ä¸²ä¸­ï¼‰
    - å…è®¸æ³¨é‡Š
    - ç­‰ç­‰
    """
    try:
        # å…ˆå°è¯•ä½¿ç”¨ json5 è§£æï¼ˆæ›´å®½æ¾ï¼‰
        return json5.loads(content)
    except Exception as e:
        # å¦‚æœ json5 å¤±è´¥ï¼Œå°è¯•æ ‡å‡† json
        try:
            return json.loads(content)
        except json.JSONDecodeError as je:
            # å¦‚æœéƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ï¼ˆè°ƒç”¨è€…å¯ä»¥ä½¿ç”¨ repair_json_with_llm ä¿®å¤ï¼‰
            raise e


async def repair_json_with_llm(invalid_json: str, expected_schema: Optional[str] = None) -> Dict[str, Any]:
    """ä½¿ç”¨ LLM ä¿®å¤æ— æ•ˆçš„ JSON å­—ç¬¦ä¸²
    
    Args:
        invalid_json: æ— æ•ˆçš„ JSON å­—ç¬¦ä¸²
        expected_schema: å¯é€‰çš„ JSON schema æè¿°ï¼Œå¸®åŠ© LLM ç†è§£æœŸæœ›çš„æ ¼å¼
    
    Returns:
        ä¿®å¤åçš„ JSON å¯¹è±¡
    """
    if MOCK_MODE or client is None:
        raise ValueError("LLM ä¸å¯ç”¨ï¼Œæ— æ³•ä¿®å¤ JSON")
    
    # æ„å»ºä¿®å¤ prompt
    schema_hint = ""
    if expected_schema:
        schema_hint = f"\næœŸæœ›çš„ JSON ç»“æ„ï¼š\n{expected_schema}"
    
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ª JSON ä¿®å¤ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†æ— æ•ˆçš„ JSON å­—ç¬¦ä¸²ä¿®å¤ä¸ºæœ‰æ•ˆçš„ JSONã€‚

è§„åˆ™ï¼š
1. åªè¿”å›ä¿®å¤åçš„ JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—
2. ä¿æŒåŸå§‹æ•°æ®çš„å«ä¹‰å’Œç»“æ„
3. ä¿®å¤å¸¸è§çš„ JSON é”™è¯¯ï¼š
   - æœªè½¬ä¹‰çš„å¼•å·
   - å°¾éšé€—å·
   - ä¸­æ–‡æ ‡ç‚¹ç¬¦å·ï¼ˆï¼Œã€ï¼šï¼‰
   - æ§åˆ¶å­—ç¬¦
   - æœªé—­åˆçš„æ‹¬å·
   - å¤šä¸ª JSON å¯¹è±¡ï¼ˆåªä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
4. ç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²å€¼éƒ½æ­£ç¡®è½¬ä¹‰
5. ç¡®ä¿æ‰€æœ‰æ•°å­—ã€å¸ƒå°”å€¼ã€null æ ¼å¼æ­£ç¡®{schema_hint}

åªè¿”å›ä¿®å¤åçš„ JSONï¼Œä¸è¦ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡æœ¬ã€‚"""

    user_prompt = f"""è¯·ä¿®å¤ä»¥ä¸‹æ— æ•ˆçš„ JSONï¼š

{invalid_json[:2000]}  # é™åˆ¶é•¿åº¦é¿å…è¶…å‡º token é™åˆ¶

åªè¿”å›ä¿®å¤åçš„ JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    try:
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # å¦‚æœä½¿ç”¨æœ¬åœ° LLMï¼Œæ£€æŸ¥å¹¶æˆªæ–­æ¶ˆæ¯
        if LOCAL_LLM:
            max_input_tokens = int(MAX_CONTEXT_LENGTH * 0.8)
            messages = truncate_messages_if_needed(messages, max_input_tokens)
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": os.getenv("OPENAI_MODEL", "gpt-4o"),
            "messages": messages,
            "temperature": 0.1  # ä½æ¸©åº¦ï¼Œç¡®ä¿ä¿®å¤å‡†ç¡®æ€§
        }
        
        if not LOCAL_LLM:
            request_params["response_format"] = {"type": "json_object"}
        else:
            request_params["max_tokens"] = min(MAX_OUTPUT_TOKENS, 1024)  # ä¿®å¤ JSON é€šå¸¸ä¸éœ€è¦å¤ªå¤š token
        
        response = await client.chat.completions.create(**request_params)
        
        # æ£€æŸ¥å“åº”
        if not response.choices or len(response.choices) == 0:
            raise ValueError("LLM å“åº”ä¸ºç©º")
        
        choice = response.choices[0]
        repaired_content = choice.message.content
        
        if repaired_content is None:
            raise ValueError("LLM ä¿®å¤åçš„å†…å®¹ä¸ºç©º")
        
        print(f"ğŸ”§ LLM å·²å°è¯•ä¿®å¤ JSONï¼Œä¿®å¤åçš„å†…å®¹é•¿åº¦: {len(repaired_content)} å­—ç¬¦")
        
        # å°è¯•è§£æä¿®å¤åçš„ JSON
        try:
            return json5.loads(repaired_content)
        except:
            try:
                return json.loads(repaired_content)
            except json.JSONDecodeError:
                # å¦‚æœä¿®å¤åä»ç„¶æ— æ•ˆï¼Œå°è¯•æå– JSON å¯¹è±¡
                json_match = re.search(r'\{.*\}', repaired_content, re.DOTALL)
                if json_match:
                    try:
                        return json5.loads(json_match.group(0))
                    except:
                        return json.loads(json_match.group(0))
                raise ValueError("LLM ä¿®å¤åçš„ JSON ä»ç„¶æ— æ•ˆ")
    
    except Exception as e:
        print(f"âŒ LLM ä¿®å¤ JSON å¤±è´¥: {e}")
        raise


MOCK_MODE = os.getenv("MOCK_MODE", "false").lower() == "true"

# æ”¯æŒæœ¬åœ° LLMï¼šå¦‚æœ LOCAL_LLM ä¸ä¸ºç©ºï¼Œä½¿ç”¨æœ¬åœ° APIï¼›å¦åˆ™ä½¿ç”¨ OpenAI
LOCAL_LLM = os.getenv("LOCAL_LLM", "").strip()

# Context length é…ç½®ï¼ˆç”¨äºæœ¬åœ° LLMï¼Œå¦‚ Qwen2.5-7Bï¼‰
MAX_CONTEXT_LENGTH = int(os.getenv("MAX_CONTEXT_LENGTH", "4096"))  # é»˜è®¤ 4096 tokens
MAX_OUTPUT_TOKENS = int(os.getenv("MAX_OUTPUT_TOKENS", "2048"))  # é»˜è®¤è¾“å‡ºæœ€å¤š 2048 tokensï¼ˆå¢åŠ ä»¥å¤„ç†å¤æ‚ JSONï¼‰


if not MOCK_MODE:
    if LOCAL_LLM:
        # ä½¿ç”¨æœ¬åœ° LLM APIï¼ˆå‡è®¾æ ¼å¼å…¼å®¹ OpenAIï¼‰
        # ç¡®ä¿ URL æ ¼å¼æ­£ç¡®ï¼ˆæ·»åŠ  /v1 å¦‚æœä¸å­˜åœ¨ï¼‰
        base_url = LOCAL_LLM.rstrip('/')
        if not base_url.endswith('/v1'):
            base_url = f"{base_url}/v1"
        
        print(f"ğŸ”§ ä½¿ç”¨æœ¬åœ° LLM API: {base_url}")
        print(f"   Context Length: {MAX_CONTEXT_LENGTH} tokens")
        print(f"   Max Output Tokens: {MAX_OUTPUT_TOKENS} tokens")
        client = AsyncOpenAI(
            api_key=os.getenv("OPENAI_API_KEY", "not-needed"),  # æœ¬åœ° LLM å¯èƒ½ä¸éœ€è¦ key
            base_url=base_url,
            timeout=120.0  # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œæœ¬åœ° LLM å¯èƒ½è¾ƒæ…¢ï¼Œç”Ÿæˆå¤æ‚ JSON éœ€è¦æ›´å¤šæ—¶é—´
        )
    else:
        # ä½¿ç”¨ OpenAI API
        print("ğŸ”§ ä½¿ç”¨ OpenAI API")
        client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
else:
    client = None
    print("ğŸ”§ ä½¿ç”¨ MOCK æ¨¡å¼")


def estimate_tokens(text: str) -> int:
    """ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡ï¼ˆä¸­æ–‡çº¦ 1-2 å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦ 4 å­—ç¬¦/tokenï¼‰"""
    # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦æ•° + è‹±æ–‡å•è¯æ•° * 1.3
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    # ä¸­æ–‡å­—ç¬¦æŒ‰ 1.5 tokens/å­—ç¬¦ï¼Œè‹±æ–‡æŒ‰ 0.25 tokens/å­—ç¬¦ä¼°ç®—
    return int(chinese_chars * 1.5 + english_chars * 0.25 + len(text) * 0.1)


def truncate_messages_if_needed(messages: List[Dict[str, str]], max_tokens: int) -> List[Dict[str, str]]:
    """å¦‚æœæ¶ˆæ¯æ€»é•¿åº¦è¶…è¿‡é™åˆ¶ï¼Œæˆªæ–­å¯¹è¯å†å²ï¼ˆä¿ç•™ system å’Œæœ€æ–°çš„ user æ¶ˆæ¯ï¼‰"""
    if not LOCAL_LLM:
        return messages  # OpenAI ä¸éœ€è¦æ‰‹åŠ¨æˆªæ–­
    
    total_tokens = sum(estimate_tokens(msg.get("content", "")) for msg in messages)
    if total_tokens <= max_tokens:
        return messages
    
    # ä¿ç•™ system æ¶ˆæ¯å’Œæœ€åä¸€ä¸ª user æ¶ˆæ¯
    system_msg = messages[0] if messages and messages[0].get("role") == "system" else None
    last_user_msg = None
    for msg in reversed(messages):
        if msg.get("role") == "user":
            last_user_msg = msg
            break
    
    # ä»ä¸­é—´çš„æ¶ˆæ¯å¼€å§‹æˆªæ–­ï¼ˆä¿ç•™æœ€è¿‘çš„å‡ æ¡ï¼‰
    truncated = []
    if system_msg:
        truncated.append(system_msg)
    
    # ä¿ç•™æœ€è¿‘çš„å‡ æ¡æ¶ˆæ¯ï¼ˆé™¤äº†æœ€åä¸€ä¸ª userï¼‰
    remaining_tokens = max_tokens - estimate_tokens(system_msg.get("content", "") if system_msg else "")
    if last_user_msg:
        remaining_tokens -= estimate_tokens(last_user_msg.get("content", ""))
    
    # ä»åå¾€å‰æ·»åŠ æ¶ˆæ¯ï¼Œç›´åˆ°è¾¾åˆ°é™åˆ¶
    for msg in reversed(messages[1:] if system_msg else messages):
        if msg == last_user_msg:
            continue
        msg_tokens = estimate_tokens(msg.get("content", ""))
        if remaining_tokens >= msg_tokens:
            truncated.insert(1, msg)  # æ’å…¥åˆ° system ä¹‹å
            remaining_tokens -= msg_tokens
        else:
            break
    
    if last_user_msg:
        truncated.append(last_user_msg)
    
    return truncated


async def generate_narrative(system_prompt: str, user_prompt: str) -> str:
    """é€šç”¨ AI æ–‡æœ¬ç”Ÿæˆ"""
    if MOCK_MODE:
        return f"[MOCK] ç³»ç»Ÿæç¤º: {system_prompt[:50]}... | ç”¨æˆ·: {user_prompt[:50]}..."
    
    messages = [
            {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    # å¦‚æœä½¿ç”¨æœ¬åœ° LLMï¼Œæ£€æŸ¥å¹¶æˆªæ–­æ¶ˆæ¯
    if LOCAL_LLM:
        # é¢„ç•™ç©ºé—´ç»™è¾“å‡ºï¼ˆçº¦ 20%ï¼‰
        max_input_tokens = int(MAX_CONTEXT_LENGTH * 0.8)
        messages = truncate_messages_if_needed(messages, max_input_tokens)
    
    request_params = {
        "model": os.getenv("OPENAI_MODEL", "gpt-4o"),
        "messages": messages,
        "temperature": 0.7
    }
    
    # æœ¬åœ° LLM è®¾ç½® max_tokens
    if LOCAL_LLM:
        request_params["max_tokens"] = MAX_OUTPUT_TOKENS
    
    response = await client.chat.completions.create(**request_params)
    
    # æ£€æŸ¥å“åº”å®Œæ•´æ€§
    if not response.choices or len(response.choices) == 0:
        raise ValueError("LLM å“åº”ä¸ºç©ºï¼šæ²¡æœ‰è¿”å›ä»»ä½•é€‰æ‹©")
    
    choice = response.choices[0]
    
    # æ£€æŸ¥ finish_reasonï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if hasattr(choice, 'finish_reason') and choice.finish_reason:
        if choice.finish_reason == "length":
            print(f"âš ï¸  è­¦å‘Šï¼šLLM å“åº”å› è¾¾åˆ° max_tokens é™åˆ¶è€Œè¢«æˆªæ–­ (finish_reason: {choice.finish_reason})")
        elif choice.finish_reason != "stop":
            print(f"âš ï¸  è­¦å‘Šï¼šLLM å“åº”å¼‚å¸¸ç»“æŸ (finish_reason: {choice.finish_reason})")
    
    content = choice.message.content
    if content is None:
        raise ValueError("LLM å“åº”å†…å®¹ä¸ºç©º")
    
    return content


def parse_content(content: str) -> Dict[str, Any]:

    # Use json5 to parse the content
    try:
        return json5.loads(content)
    except json.JSONDecodeError:
        raise ValueError("æ— æ³•è§£æå†…å®¹ä¸º JSON")
    
    # If not json, return the content as is
    return content

async def generate_json(system_prompt: str, user_prompt: str, schema_hint: str = "") -> Dict[str, Any]:
    """ç”Ÿæˆç»“æ„åŒ– JSON è¾“å‡º
    
    Args:
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        user_prompt: ç”¨æˆ·æç¤ºè¯
        schema_hint: JSON schema æç¤ºï¼Œç”¨äº LLM ä¿®å¤æ—¶æä¾›æœŸæœ›æ ¼å¼
    """
    if MOCK_MODE:
        # Mock è¿”å›ç¤ºä¾‹æ•°æ®
        return {
            "choices": [
                {"id": "1", "text": "[MOCK] é€‰é¡¹ A: ç»§ç»­è°ƒæŸ¥"},
                {"id": "2", "text": "[MOCK] é€‰é¡¹ B: ç¦»å¼€è¿™é‡Œ"},
                {"id": "3", "text": "[MOCK] é€‰é¡¹ C: ä¸ NPC äº¤è°ˆ"}
            ],
            "narrative": "[MOCK] è¿™æ˜¯ä¸€æ®µå™äº‹æ–‡æœ¬...",
            "mood": "neutral",
            "character_positions": {
                "player": "right"
            }
        }
    
    full_system = f"{system_prompt}\n\nä½ å¿…é¡»åªè¿”å›æœ‰æ•ˆçš„ JSONã€‚{schema_hint}"
    
    try:
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": full_system},
            {"role": "user", "content": user_prompt}
        ]
        
        # å¦‚æœä½¿ç”¨æœ¬åœ° LLMï¼Œæ£€æŸ¥å¹¶æˆªæ–­æ¶ˆæ¯
        if LOCAL_LLM:
            # é¢„ç•™ç©ºé—´ç»™è¾“å‡ºï¼ˆçº¦ 20%ï¼‰
            max_input_tokens = int(MAX_CONTEXT_LENGTH * 0.8)
            messages = truncate_messages_if_needed(messages, max_input_tokens)
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": os.getenv("OPENAI_MODEL", "gpt-4o"),
            "messages": messages,
            "temperature": 0.7
        }
        # æœ¬åœ° LLM å¯èƒ½ä¸æ”¯æŒ response_formatï¼Œå®Œå…¨ä¸ä¼ é€’è¯¥å‚æ•°
        if not LOCAL_LLM:
            request_params["response_format"] = {"type": "json_object"}
        else:
            # æœ¬åœ° LLM è®¾ç½® max_tokens
            request_params["max_tokens"] = MAX_OUTPUT_TOKENS
        
        response = await client.chat.completions.create(**request_params)
        
        # æ£€æŸ¥å“åº”å®Œæ•´æ€§
        if not response.choices or len(response.choices) == 0:
            raise ValueError("LLM å“åº”ä¸ºç©ºï¼šæ²¡æœ‰è¿”å›ä»»ä½•é€‰æ‹©")
        
        choice = response.choices[0]
        
        # æ£€æŸ¥ finish_reasonï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(choice, 'finish_reason') and choice.finish_reason:
            if choice.finish_reason == "length":
                print(f"âš ï¸  è­¦å‘Šï¼šLLM å“åº”å› è¾¾åˆ° max_tokens é™åˆ¶è€Œè¢«æˆªæ–­ (finish_reason: {choice.finish_reason})")
                print(f"   å½“å‰ max_tokens: {request_params.get('max_tokens', 'N/A')}")
            elif choice.finish_reason != "stop":
                print(f"âš ï¸  è­¦å‘Šï¼šLLM å“åº”å¼‚å¸¸ç»“æŸ (finish_reason: {choice.finish_reason})")
        
        content = choice.message.content
        print("--------------------------------")
        print(f"content: {content}")
        print("--------------------------------")
        
        if content is None:
            raise ValueError("LLM å“åº”å†…å®¹ä¸ºç©º")
        
        try:
            parsed_content = parse_content(content)
            return parsed_content
        except Exception as json_err:
            # JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤
            if LOCAL_LLM:
                print(f"âš ï¸  JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {json_err}")
                print(f"   åŸå§‹å†…å®¹å‰ 300 å­—ç¬¦: {content[:300]}")
                print(f"   å®Œæ•´å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"   å®Œæ•´å†…å®¹:\n{content}")
                print(f"   {'='*80}")
                
    except Exception as e:
        error_msg = str(e)
        if LOCAL_LLM:
            print(f"âŒ æœ¬åœ° LLM è¿æ¥é”™è¯¯: {error_msg}")
            print(f"   è¯·æ£€æŸ¥:")
            print(f"   1. LOCAL_LLM={LOCAL_LLM} æ˜¯å¦æ­£ç¡®")
            print(f"   2. æœ¬åœ° LLM æœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ")
            print(f"   3. URL æ˜¯å¦å¯ä»¥è®¿é—®ï¼ˆå°è¯•: curl {LOCAL_LLM.rstrip('/')}/v1/modelsï¼‰")
        else:
            print(f"âŒ OpenAI API è¿æ¥é”™è¯¯: {error_msg}")
        raise


async def generate_npc_response(
    npc_name: str,
    npc_personality: str,
    npc_description: str,
    scenario: Optional[str],
    example_dialogs: List[str],
    conversation_history: List[Dict[str, str]],
    player_message: str,
    world_context: str
) -> Dict[str, Any]:
    """NPC ç‹¬ç«‹äººæ ¼å¯¹è¯ç”Ÿæˆ"""
    
    # æ„å»º NPC ç³»ç»Ÿæç¤º
    if LOCAL_LLM:
        # ç®€åŒ–ç‰ˆï¼Œé’ˆå¯¹æœ¬åœ°å°æ¨¡å‹ï¼ˆå¦‚ Qwen2.5-7Bï¼‰ï¼Œå¼ºè°ƒåªè¿”å›å•ä¸ª JSON
        system_prompt = f"""!!!æœ€é‡è¦çš„ï¼šè¿”å›çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªJSONæ ¼å¼!!!
ä½ æ˜¯ {npc_name}ï¼Œä¸€ä¸ª MUD æ¸¸æˆä¸­çš„è§’è‰²ã€‚
æ€§æ ¼ç‰¹ç‚¹: {npc_personality}
å¤–è²Œæè¿°: {npc_description}
{f'èƒŒæ™¯æ•…äº‹: {scenario}' if scenario else ''}
{f'å¯¹è¯é£æ ¼ç¤ºä¾‹:{chr(10).join(example_dialogs[:3])}' if example_dialogs else ''}
ä¸–ç•ŒèƒŒæ™¯: {world_context}

è¯·åªè¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸”åªè¿”å› JSONã€‚

JSON æ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
{{
  "response": "ä½ çš„è§’è‰²å›å¤ï¼ˆå¯ä»¥åŒ…å«*åŠ¨ä½œ*å’Œã€å¯¹è¯ã€ï¼‰",
  "emotion": "{'|'.join(EMOTION_LIST)}",
  "relationship_change": -5 åˆ° +5 çš„æ•´æ•°,
  "internal_thought": "ç®€çŸ­çš„å†…å¿ƒç‹¬ç™½"
}}

é‡è¦è§„åˆ™ï¼š
1. å¿…é¡»è¿”å›ä¸€ä¸ª JSON å¯¹è±¡
2. ä¸è¦è¿”å›ä»»ä½• JSON ä¹‹å¤–çš„æ–‡æœ¬
3. ä¿è¯å­—æ®µé½å…¨ï¼Œå­—æ®µåä¸è¦æ”¹åŠ¨
4. æƒ…ç»ªä»…ä½¿ç”¨ä¸Šè¿°æšä¸¾å€¼ä¹‹ä¸€
5. relationship_change å¿…é¡»æ˜¯æ•´æ•°
6. response é‡Œçš„ "å¯¹è¯" å‰åçš„ä¸€å®šè¦ç”¨ä¸­æ–‡ç›´è§’åŒå¼•å·ã€ã€

ç¤ºä¾‹1ï¼ˆè¯·ä¸¥æ ¼å‚è€ƒæ ¼å¼ï¼‰ï¼š
{{
  "response": "*å¾®ç¬‘* ã€å¥½çš„ï¼Œæˆ‘æ¥å¸®ä½ ã€‚ã€",
  "emotion": "happy",
  "relationship_change": 1,
  "internal_thought": "ä»–çœ‹èµ·æ¥å€¼å¾—ä¿¡ä»»ã€‚"
}}"""
    else:
        # è¯¦ç»†ç‰ˆï¼ˆOpenAI ç­‰ï¼‰
        system_prompt = f"""ä½ æ˜¯ {npc_name}ï¼Œä¸€ä¸ª MUD æ¸¸æˆä¸­çš„è§’è‰²ã€‚è¯·ç”¨ä¸­æ–‡å›å¤ã€‚

æ€§æ ¼ç‰¹ç‚¹: {npc_personality}

å¤–è²Œæè¿°: {npc_description}

{f'èƒŒæ™¯æ•…äº‹: {scenario}' if scenario else ''}

{f'å¯¹è¯é£æ ¼ç¤ºä¾‹:{chr(10).join(example_dialogs[:3])}' if example_dialogs else ''}

ä¸–ç•ŒèƒŒæ™¯: {world_context}

ç©å®¶è¾“å…¥æ ¼å¼è¯´æ˜ï¼š
- *æ˜Ÿå·åŒ…è£¹* = ç©å®¶çš„åŠ¨ä½œï¼ˆä¾‹å¦‚ï¼š*å¾®å¾®ç‚¹å¤´*ï¼‰
- "åŒå¼•å·" = ç©å®¶è¯´çš„è¯ï¼ˆä¾‹å¦‚ï¼šã€ä½ å¥½ã€"ï¼‰
- ï¼ˆåœ†æ‹¬å·ï¼‰= ç©å®¶ç»™AIçš„æŒ‡ç¤ºï¼Œä¸æ˜¯è§’è‰²å¯¹è¯
- ~æ³¢æµªå·~ = æ‹–é•¿éŸ³

è§„åˆ™:
- å®Œå…¨ä¿æŒ {npc_name} çš„è§’è‰²
- ä½ çš„å›å¤åº”è¯¥åæ˜ ä½ çš„æ€§æ ¼ç‰¹ç‚¹
- ä¿æŒç®€æ´ï¼ˆé€šå¸¸2-4å¥è¯ï¼‰
- ä½ å¯ä»¥è¡¨è¾¾ä¼šå½±å“ç«‹ç»˜çš„æƒ…ç»ª
- ç†è§£ç©å®¶çš„åŠ¨ä½œå¹¶åšå‡ºç›¸åº”ååº”

ä½ çš„å›å¤æ ¼å¼ï¼š
- ç”¨ *æ˜Ÿå·* åŒ…è£¹ä½ çš„åŠ¨ä½œå’Œè¡¨æƒ…
- ç”¨ "ä¸­æ–‡åŒå¼•å·" æˆ–ä¸å¸¦å¼•å·ç›´æ¥å›å¤å¯¹è¯

ç”¨ JSON æ ¼å¼å›å¤:
{{
    "response": "ä½ çš„è§’è‰²å†…å›å¤ï¼ˆå¯æ··åˆåŠ¨ä½œå’Œå¯¹è¯ï¼Œå¦‚ï¼š*å¾®ç¬‘* ã€å½“ç„¶å¯ä»¥ã€ï¼‰",
    "emotion": "{'|'.join(EMOTION_LIST)}",
    "relationship_change": -5 åˆ° +5ï¼ˆè¿™æ¬¡äº’åŠ¨å¦‚ä½•å½±å“ä½ å¯¹ç©å®¶çš„æ„Ÿè§‰ï¼‰,
    "internal_thought": "ç®€çŸ­çš„å†…å¿ƒç‹¬ç™½ï¼ˆä¸ä¼šæ˜¾ç¤ºç»™ç©å®¶ï¼‰"
}}"""

    # æ„å»ºå¯¹è¯å†å²
    messages = [{"role": "system", "content": system_prompt}]
    # é™åˆ¶å¯¹è¯å†å²é•¿åº¦ï¼ˆæ ¹æ® context length åŠ¨æ€è°ƒæ•´ï¼‰
    history_limit = 20 if not LOCAL_LLM else 10  # æœ¬åœ° LLM ä½¿ç”¨æ›´å°‘çš„å†å²
    for msg in conversation_history[-history_limit:]:  # æœ€è¿‘ N æ¡
        role = "assistant" if msg["role"] == "npc" else "user"
        messages.append({"role": role, "content": msg["content"]})
    messages.append({"role": "user", "content": player_message})
    
    if MOCK_MODE:
        return {
            "response": f"[MOCK] {npc_name}: æˆ‘å¬åˆ°ä½ è¯´äº†ã€Œ{player_message[:20]}...ã€",
            "emotion": "default",
            "relationship_change": 0,
            "internal_thought": "[MOCK] å†…å¿ƒæƒ³æ³•..."
        }
    
    # å¦‚æœä½¿ç”¨æœ¬åœ° LLMï¼Œæ£€æŸ¥å¹¶æˆªæ–­æ¶ˆæ¯
    if LOCAL_LLM:
        # é¢„ç•™ç©ºé—´ç»™è¾“å‡ºï¼ˆçº¦ 20%ï¼‰
        max_input_tokens = int(MAX_CONTEXT_LENGTH * 0.8)
        messages = truncate_messages_if_needed(messages, max_input_tokens)
    
    # æ„å»ºè¯·æ±‚å‚æ•°
    request_params = {
        "model": os.getenv("OPENAI_MODEL", "gpt-4o"),
        "messages": messages,
        "temperature": 0.8
    }
    # æœ¬åœ° LLM å¯èƒ½ä¸æ”¯æŒ response_formatï¼Œå®Œå…¨ä¸ä¼ é€’è¯¥å‚æ•°
    if not LOCAL_LLM:
        request_params["response_format"] = {"type": "json_object"}
    else:
        # æœ¬åœ° LLM è®¾ç½® max_tokens
        request_params["max_tokens"] = MAX_OUTPUT_TOKENS
    
    response = await client.chat.completions.create(**request_params)
    
    # æ£€æŸ¥å“åº”å®Œæ•´æ€§
    if not response.choices or len(response.choices) == 0:
        raise ValueError("LLM å“åº”ä¸ºç©ºï¼šæ²¡æœ‰è¿”å›ä»»ä½•é€‰æ‹©")
    
    choice = response.choices[0]
    
    # æ£€æŸ¥ finish_reasonï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if hasattr(choice, 'finish_reason') and choice.finish_reason:
        if choice.finish_reason == "length":
            print(f"âš ï¸  è­¦å‘Šï¼šLLM å“åº”å› è¾¾åˆ° max_tokens é™åˆ¶è€Œè¢«æˆªæ–­ (finish_reason: {choice.finish_reason})")
            print(f"   å½“å‰ max_tokens: {request_params.get('max_tokens', 'N/A')}")
        elif choice.finish_reason != "stop":
            print(f"âš ï¸  è­¦å‘Šï¼šLLM å“åº”å¼‚å¸¸ç»“æŸ (finish_reason: {choice.finish_reason})")
    
    content = choice.message.content
    print("--------------------------------")
    print(f"NPC conversation content: {content}")
    print("--------------------------------")
    if content is None:
        raise ValueError("LLM å“åº”å†…å®¹ä¸ºç©º")

    # å¦‚æœæœ¬åœ° LLM è¿”å›äº†å¤šä¸ª JSON å¯¹è±¡ï¼Œå–ç¬¬ä¸€ä¸ª
    if LOCAL_LLM:
        json_matches = re.findall(r'\{.*?\}', content, re.DOTALL)
        if json_matches:
            if len(json_matches) > 1:
                print(f"âš ï¸  å‘ç°å¤šä¸ª JSON å¯¹è±¡ï¼Œå·²å–ç¬¬ä¸€ä¸ªï¼Œæ€»æ•°: {len(json_matches)}")
            content = json_matches[0]
    
    try:
        return parse_json_with_fallback(content)
    except Exception as json_err:
        print(f"âš ï¸  JSON è§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ LLM ä¿®å¤: {json_err}")
        try:
            # å°è¯•ä½¿ç”¨ LLM ä¿®å¤ JSON
            return await repair_json_with_llm(content, expected_schema="NPC å¯¹è¯å“åº”æ ¼å¼ï¼š{\"response\": \"...\", \"emotion\": \"...\", \"relationship_change\": æ•°å­—, \"internal_thought\": \"...\"}")
        except Exception as repair_err:
            print(f"âŒ  LLM ä¿®å¤ä¹Ÿå¤±è´¥: {repair_err}")
            raise json_err


async def generate_choices(
    world_rules: List[str],
    current_situation: str,
    recent_events: List[str],
    player_stats: Dict[str, Any],
    available_actions: List[str],
    npcs_in_scene: List[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """ç”Ÿæˆç©å®¶é€‰é¡¹ï¼ŒåŒæ—¶å†³å®šè§’è‰²åœ¨åœºæ™¯ä¸­çš„ä½ç½®"""
    
    # æ„å»º NPC ä¿¡æ¯
    npc_info = ""
    if npcs_in_scene:
        npc_names = [npc.get("name", "æœªçŸ¥") for npc in npcs_in_scene]
        npc_info = f"\nå½“å‰åœºæ™¯ä¸­çš„ NPC: {', '.join(npc_names)}"
    
    # é’ˆå¯¹æœ¬åœ° LLMï¼ˆå¦‚ Qwen2.5-7Bï¼‰ä½¿ç”¨æ›´ç®€å•ã€æ›´æ˜ç¡®çš„ prompt
    if LOCAL_LLM:
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ¸¸æˆç³»ç»Ÿï¼Œå¿…é¡»è¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚

ä»»åŠ¡ï¼šç”Ÿæˆ 3-4 ä¸ªæ¸¸æˆé€‰é¡¹ï¼Œå¹¶å®‰æ’è§’è‰²ä½ç½®ã€‚

JSON æ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
{{
  "narrative": "ç®€çŸ­æè¿°å½“å‰æƒ…å¢ƒ",
  "choices": [
    {{"id": "1", "text": "é€‰é¡¹1çš„æ–‡å­—", "hint": "æç¤ºæˆ–null"}},
    {{"id": "2", "text": "é€‰é¡¹2çš„æ–‡å­—", "hint": null}},
    {{"id": "3", "text": "é€‰é¡¹3çš„æ–‡å­—", "hint": null}}
  ],
  "mood": "neutral",
  "character_positions": {{
    "player": "left"
  }}
}}

é‡è¦è§„åˆ™ï¼š
1. åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—
2. text å­—æ®µå¿…é¡»æ˜¯çº¯ä¸­æ–‡æ–‡æœ¬ï¼Œä¸è¦ä»£ç 
3. id å¿…é¡»æ˜¯å­—ç¬¦ä¸² "1", "2", "3" ç­‰
4. mood å¿…é¡»æ˜¯: {'|'.join(EMOTION_LIST)} ä¹‹ä¸€
5. character_positions ä¸­ player å¿…é¡»æ˜¯: left, center, right ä¹‹ä¸€
6. å¦‚æœæœ‰ NPCï¼Œæ·»åŠ  "npc_id": "left|center|right"
7. hint å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ– null

ç¤ºä¾‹ï¼ˆä¸¥æ ¼æŒ‰ç…§è¿™ä¸ªæ ¼å¼ï¼‰ï¼š
{{
  "narrative": "ä½ ç«™åœ¨éœ“è™¹ç¯ä¸‹ï¼Œæ€è€ƒä¸‹ä¸€æ­¥è¡ŒåŠ¨",
  "choices": [
    {{"id": "1", "text": "ç»§ç»­å‰è¿›", "hint": null}},
    {{"id": "2", "text": "è§‚å¯Ÿå‘¨å›´", "hint": "å¯èƒ½å‘ç°çº¿ç´¢"}},
    {{"id": "3", "text": "è¿”å›", "hint": null}}
  ],
  "mood": "neutral",
  "character_positions": {{
    "player": "center"
  }}
}}"""
    else:
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ª MUD æ¸¸æˆçš„æ¸¸æˆå¤§å¸ˆã€‚ä¸ºç©å®¶ç”Ÿæˆæœ‰æ„ä¹‰çš„é€‰é¡¹ï¼Œå¹¶åƒè§†è§‰å°è¯´å¯¼æ¼”ä¸€æ ·å®‰æ’è§’è‰²åœ¨ç”»é¢ä¸­çš„ä½ç½®ã€‚è¯·ç”¨ä¸­æ–‡å›å¤ã€‚

è§„åˆ™:
- ç”Ÿæˆ 3-4 ä¸ªä¸åŒçš„ã€æœ‰æ„ä¹‰çš„é€‰é¡¹
- æ¯ä¸ªé€‰é¡¹åº”è¯¥å¯¼å‘ä¸åŒçš„å™äº‹è·¯å¾„
- é€‰é¡¹åº”è¯¥ç¬¦åˆä¸–ç•Œè§„åˆ™
- è‡³å°‘åŒ…å«ä¸€ä¸ªã€Œå®‰å…¨ã€é€‰é¡¹å’Œä¸€ä¸ªã€Œå†’é™©ã€é€‰é¡¹
- é€‰é¡¹åº”è¯¥åœ¨å½“å‰æƒ…å¢ƒä¸‹æ„Ÿè§‰è‡ªç„¶
- è€ƒè™‘ç©å®¶çš„è´§å¸çŠ¶å†µï¼Œå¦‚æœé€‰é¡¹æ¶‰åŠæ¶ˆè´¹ï¼Œåœ¨ hint ä¸­æç¤ºéœ€è¦çš„è´§å¸ç±»å‹å’Œæ•°é‡

é‡è¦ï¼šé€‰é¡¹æ–‡æœ¬æ ¼å¼è¦æ±‚:
- "text" å­—æ®µå¿…é¡»æ˜¯çº¯æ–‡æœ¬æè¿°ï¼Œç»™ç©å®¶çœ‹çš„é€‰é¡¹æ–‡å­—
- ç»å¯¹ä¸è¦åŒ…å«ä»»ä½•ä»£ç ï¼ˆJavaScriptã€Python ç­‰ï¼‰
- ç»å¯¹ä¸è¦åŒ…å«å­—ç¬¦ä¸²è¿æ¥æ“ä½œç¬¦ï¼ˆ+ï¼‰æˆ–å‡½æ•°è°ƒç”¨
- ç»å¯¹ä¸è¦åŒ…å«æ¡ä»¶è¡¨è¾¾å¼æˆ–é€»è¾‘åˆ¤æ–­
- é€‰é¡¹æ–‡æœ¬åº”è¯¥æ˜¯ç®€å•çš„ã€å¯è¯»çš„ä¸­æ–‡æè¿°
- ä¾‹å¦‚ï¼š"ç»§ç»­ä¸“æ³¨äºæ—§æ•°æ®æ¿ä¸Šçš„ä¿¡æ¯æµ" âœ…
- é”™è¯¯ç¤ºä¾‹ï¼š"ç»§ç»­ä¸“æ³¨äºæ—§æ•°æ®æ¿\" + (player().inventory.includes(\"æ—§æ•°æ®æ¿\") ? ...)" âŒ
- é€»è¾‘å¤„ç†ç”±åç«¯å®Œæˆï¼Œä½ åªéœ€è¦æä¾›çº¯æ–‡æœ¬é€‰é¡¹æè¿°

ç»æµç³»ç»Ÿç†è§£:
- æ¸¸æˆå†…è´§å¸ï¼ˆå¦‚"é‡‘å¸"ï¼‰ï¼šç”¨äºè´­ä¹°æ¸¸æˆé€»è¾‘å†…çš„ç‰©å“ã€æœåŠ¡ã€é£Ÿç‰©ç­‰
- ä»˜è´¹è´§å¸ï¼ˆå¦‚"å®çŸ³"ï¼‰ï¼šç”¨äºè´­ä¹°ä¸å½±å“æ¸¸æˆå¹³è¡¡çš„é“å…·ï¼Œå¦‚çš®è‚¤ã€é…é¥°ã€è£…é¥°å“ç­‰
- æ ¹æ®è´§å¸è§„åˆ™åˆ¤æ–­æ¶ˆè´¹ç±»å‹ï¼Œåœ¨é€‰é¡¹ hint ä¸­æ˜ç¡®è¯´æ˜

è§’è‰²ä½ç½®è§„åˆ™ï¼ˆåƒè§†è§‰å°è¯´ä¸€æ ·ï¼‰:
- ä½ç½®æœ‰ä¸‰ä¸ªï¼šleftï¼ˆå·¦ï¼‰ã€centerï¼ˆä¸­ï¼‰ã€rightï¼ˆå³ï¼‰
- ç©å®¶ï¼ˆplayerï¼‰å’Œ NPC åº”è¯¥æ ¹æ®å‰§æƒ…å…³ç³»å’Œå¯¹è¯æƒ…å¢ƒå®‰æ’ä½ç½®
- å¯¹è¯æ—¶ï¼ŒåŒæ–¹é€šå¸¸é¢å¯¹é¢ï¼ˆä¸€å·¦ä¸€å³ï¼‰
- é‡è¦è§’è‰²æˆ–æ­£åœ¨è¯´è¯çš„è§’è‰²å¯ä»¥åœ¨ä¸­é—´
- å¤šä¸ªè§’è‰²æ—¶è¦åˆç†åˆ†å¸ƒ

ç”¨ JSON æ ¼å¼å›å¤:
{{
    "narrative": "å½“å‰æ—¶åˆ»/æƒ…å¢ƒçš„ç®€çŸ­æè¿°",
    "choices": [
        {{"id": "1", "text": "é€‰é¡¹æè¿°ï¼ˆçº¯æ–‡æœ¬ï¼Œæ— ä»£ç ï¼‰", "hint": "å…³äºåæœçš„å¯é€‰æç¤º"}},
        {{"id": "2", "text": "é€‰é¡¹æè¿°ï¼ˆçº¯æ–‡æœ¬ï¼Œæ— ä»£ç ï¼‰", "hint": null}},
        ...
    ],
    "mood": "{'|'.join(EMOTION_LIST)}",
    "character_positions": {{
        "player": "left|center|right",
        "npc_id_1": "left|center|right",
        "npc_id_2": "left|center|right"
    }}
}}"""

    # é’ˆå¯¹æœ¬åœ° LLM ä½¿ç”¨æ›´ç®€æ´çš„ user_prompt
    if LOCAL_LLM:
        user_prompt = f"""ç”Ÿæˆæ¸¸æˆé€‰é¡¹ã€‚

ä¸–ç•Œè§„åˆ™: {', '.join(world_rules[:3]) if world_rules else 'æ— ç‰¹æ®Šè§„åˆ™'}

å½“å‰æƒ…å¢ƒ: {current_situation[:200]}{npc_info[:100]}

ç©å®¶çŠ¶æ€: è´§å¸={player_stats.get('currency', 0)}, å®çŸ³={player_stats.get('gems', 0)}

{f'NPCåˆ—è¡¨: {[npc.get("id") for npc in npcs_in_scene]}' if npcs_in_scene else 'æ— NPC'}

è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¿”å›ï¼Œåªè¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""
    else:
        user_prompt = f"""ä¸–ç•Œè§„åˆ™:
{chr(10).join(f'- {rule}' for rule in world_rules)}

å½“å‰æƒ…å¢ƒ:
{current_situation}{npc_info}

æœ€è¿‘äº‹ä»¶:
{chr(10).join(f'- {event}' for event in recent_events[-5:])}

ç©å®¶çŠ¶æ€:
{json.dumps(player_stats, indent=2, ensure_ascii=False)}

å¯ç”¨è¡ŒåŠ¨ï¼ˆç‰©ç†ä¸Šå¯èƒ½çš„ï¼‰:
{chr(10).join(f'- {action}' for action in available_actions)}

{f'åœºæ™¯ä¸­çš„ NPC ID åˆ—è¡¨: {[npc.get("id") for npc in npcs_in_scene]}' if npcs_in_scene else 'åœºæ™¯ä¸­æ²¡æœ‰ NPC'}

ä¸ºç©å®¶ç”Ÿæˆåˆé€‚çš„é€‰é¡¹ï¼Œå¹¶å®‰æ’è§’è‰²çš„ç”»é¢ä½ç½®ã€‚"""

    return await generate_json(system_prompt, user_prompt)


# RP æ ¼å¼è¯´æ˜ï¼ˆä¾› AI ç†è§£ç©å®¶è¾“å…¥ï¼‰
RP_FORMAT_GUIDE = """
ç©å®¶è¾“å…¥æ ¼å¼è¯´æ˜ï¼š
- *æ˜Ÿå·åŒ…è£¹* = åŠ¨ä½œæˆ–åœºæ™¯æå†™ï¼ˆä¾‹å¦‚ï¼š*ç¼“ç¼“èµ°è¿‘ï¼Œçœ¼ç¥è­¦æƒ•*ï¼‰
- ã€ä¸­æ–‡ç›´è§’åŒå¼•å·ã€ = è§’è‰²è¯´çš„è¯ï¼ˆä¾‹å¦‚ï¼šã€ä½ æ˜¯è°ï¼Ÿã€ï¼‰
- ï¼ˆåœ†æ‹¬å·ï¼‰= ç©å®¶æ„å›¾/OOCæŒ‡ä»¤ï¼ˆä¾‹å¦‚ï¼šï¼ˆæˆ‘æƒ³å»é…’å§æ‰¾çº¿ç´¢ï¼‰ï¼‰
- ~æ³¢æµªå·~ = æ‹–é•¿éŸ³æˆ–ç‰¹æ®Šè¯­æ°”ï¼ˆä¾‹å¦‚ï¼šã€ç­‰ä¸€ä¸‹~ã€ï¼‰
- **åŒæ˜Ÿå·** = é‡ç‚¹å¼ºè°ƒ

ç©å®¶å¯èƒ½æ··åˆä½¿ç”¨è¿™äº›æ ¼å¼ï¼Œä¾‹å¦‚ï¼š
*èµ°å‘é…’ä¿* ã€æ¥æ¯æœ€çƒˆçš„ã€‚ã€ *æŠŠé’±æ‹åœ¨æ¡Œä¸Š*

ä½ éœ€è¦ç†è§£è¿™äº›æ ¼å¼ï¼Œå¹¶æ ¹æ®ç©å®¶çš„æ„å›¾åšå‡ºå“åº”ã€‚
"""


async def suggest_scene_npcs(
    scene_name: str,
    scene_description: str,
    story_context: str,
    available_characters: List[Dict[str, Any]],
    current_npcs: List[str] = None
) -> Dict[str, Any]:
    """
    æ ¹æ®åœºæ™¯å’Œå‰§æƒ…ï¼Œå»ºè®®åº”è¯¥å‡ºç°çš„ NPC
    
    ç”¨äºï¼š
    - åœºæ™¯åˆ‡æ¢æ—¶å†³å®šåŠ è½½å“ªäº›è§’è‰²
    - å‰§æƒ…å‘å±•æ—¶å¼•å…¥æ–°è§’è‰²
    """
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ¸¸æˆå‰§æƒ…å¯¼æ¼”ã€‚æ ¹æ®åœºæ™¯å’Œæ•…äº‹å‘å±•ï¼Œå»ºè®®åº”è¯¥å‡ºç°å“ªäº›è§’è‰²ã€‚è¯·ç”¨ä¸­æ–‡å›å¤ã€‚

è§„åˆ™ï¼š
- è€ƒè™‘åœºæ™¯ç±»å‹å’Œæ°›å›´
- è€ƒè™‘å‰§æƒ…å‘å±•çš„éœ€è¦
- ä¸è¦æ·»åŠ å¤ªå¤šè§’è‰²ï¼ˆ1-3 ä¸ªä¸ºå®œï¼‰
- å¦‚æœæœ‰åˆé€‚çš„ç°æœ‰è§’è‰²ï¼Œä¼˜å…ˆä½¿ç”¨
- åªæœ‰åœ¨å¿…è¦æ—¶æ‰å»ºè®®åˆ›å»ºæ–°è§’è‰²

ç”¨ JSON æ ¼å¼å›å¤:
{
    "should_add_npcs": true/false,
    "reasoning": "ä¸ºä»€ä¹ˆéœ€è¦/ä¸éœ€è¦æ·»åŠ è§’è‰²",
    "suggested_npcs": [
        {
            "action": "use_existing" æˆ– "create_new",
            "character_id": "å¦‚æœä½¿ç”¨ç°æœ‰è§’è‰²ï¼Œå¡«å†™ ID",
            "role": "è§’è‰²åœ¨å‰§æƒ…ä¸­çš„ä½œç”¨ï¼Œå¦‚ï¼šæœåŠ¡å‘˜ã€ç¥ç§˜äºº",
            "entrance": "è§’è‰²å‡ºåœºæ–¹å¼æè¿°",
            "new_character": {  // åªæœ‰ create_new æ—¶éœ€è¦
                "name": "è§’è‰²å",
                "description": "å¤–è²Œæè¿°",
                "personality": "æ€§æ ¼",
                "first_message": "å¼€åœºç™½"
            }
        }
    ]
}"""

    # æ„å»ºå¯ç”¨è§’è‰²åˆ—è¡¨
    chars_text = "æ— å¯ç”¨è§’è‰²"
    if available_characters:
        chars_list = [
            f"- {c.get('id')}: {c.get('name')} ({c.get('description', '')[:50]}...)"
            for c in available_characters[:10]
        ]
        chars_text = "\n".join(chars_list)
    
    current_text = "æ— "
    if current_npcs:
        current_text = ", ".join(current_npcs)
    
    user_prompt = f"""åœºæ™¯ï¼š{scene_name}
åœºæ™¯æè¿°ï¼š{scene_description}

æ•…äº‹ä¸Šä¸‹æ–‡ï¼š
{story_context}

å½“å‰åœºæ™¯ä¸­çš„è§’è‰²ï¼š{current_text}

å¯ç”¨çš„è§’è‰²åº“ï¼š
{chars_text}

è¿™ä¸ªåœºæ™¯åº”è¯¥æœ‰å“ªäº›è§’è‰²ï¼Ÿ"""

    if MOCK_MODE:
        return {
            "should_add_npcs": False,
            "reasoning": "[MOCK] å½“å‰åœºæ™¯ä¸éœ€è¦é¢å¤–è§’è‰²",
            "suggested_npcs": []
        }
    
    return await generate_json(system_prompt, user_prompt)


async def judge_action(
    world_rules: List[str],
    current_situation: str,
    player_action: str,
    physical_constraints: List[str]
) -> Dict[str, Any]:
    """Judge æ¨¡å—ï¼šæ ¡éªŒç©å®¶è‡ªç”±è¾“å…¥æ˜¯å¦åˆæ³•"""
    
    system_prompt = f"""ä½ æ˜¯ MUD æ¸¸æˆçš„è§„åˆ™æ‰§è¡Œè€…ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ç©å®¶çš„è¡ŒåŠ¨æ˜¯å¦è¢«å…è®¸ã€‚è¯·ç”¨ä¸­æ–‡å›å¤ã€‚

{RP_FORMAT_GUIDE}

æ‹’ç»çš„æ ‡å‡†:
1. è¿åæ˜ç¡®çš„ä¸–ç•Œè§„åˆ™
2. åœ¨å½“å‰çº¦æŸä¸‹ç‰©ç†ä¸Šä¸å¯èƒ½
3. è¯•å›¾æ“çºµæ¸¸æˆç³»ç»Ÿï¼ˆå…ƒæ¸¸æˆï¼‰
4. ä¸å½“å†…å®¹

å…è®¸çš„æ ‡å‡†:
1. åˆ›æ„ä½†åˆç†çš„è¡ŒåŠ¨
2. ç¬¦åˆä¸–ç•Œç²¾ç¥çš„è¡ŒåŠ¨
3. æ„æƒ³ä¸åˆ°ä½†æœ‰æ•ˆçš„ç©å®¶èƒ½åŠ¨æ€§

å¯¹åˆ›æ„è¡ŒåŠ¨è¦å®½å®¹ï¼Œä½†å¯¹è§„åˆ™è¿åè¦ä¸¥æ ¼ã€‚
åœ†æ‹¬å·ï¼ˆï¼‰ä¸­çš„å†…å®¹æ˜¯ç©å®¶çš„OOCæ„å›¾ï¼Œåº”è¯¥å°Šé‡ä½†è½¬åŒ–ä¸ºæ¸¸æˆå†…è¡ŒåŠ¨ã€‚

ç”¨ JSON å›å¤:
{{
    "allowed": true/false,
    "reason": "å¦‚æœæ‹’ç»ï¼Œè¯´æ˜åŸå› ",
    "suggested_action": "å¦‚æœæ‹’ç»ï¼Œç»™å‡ºæ›¿ä»£å»ºè®®ï¼Œå¦‚æœå…è®¸åˆ™ä¸º null",
    "modified_action": "å¦‚æœå…è®¸ï¼Œæ¸…ç†åçš„è¡ŒåŠ¨ç‰ˆæœ¬",
    "parsed_intent": {{
        "actions": ["è§£æå‡ºçš„åŠ¨ä½œåˆ—è¡¨"],
        "dialogues": ["è§£æå‡ºçš„å¯¹è¯åˆ—è¡¨"],
        "ooc_intent": "ç©å®¶çš„OOCæ„å›¾ï¼ˆå¦‚æœæœ‰ï¼‰"
    }}
}}"""

    user_prompt = f"""ä¸–ç•Œè§„åˆ™:
{chr(10).join(f'- {rule}' for rule in world_rules)}

å½“å‰æƒ…å¢ƒ:
{current_situation}

ç‰©ç†çº¦æŸ:
{chr(10).join(f'- {c}' for c in physical_constraints)}

ç©å®¶å°è¯•çš„è¡ŒåŠ¨:
ã€Œ{player_action}ã€

è§£æç©å®¶çš„è¾“å…¥æ ¼å¼ï¼Œåˆ¤æ–­è¿™ä¸ªè¡ŒåŠ¨æ˜¯å¦å…è®¸ã€‚"""

    if MOCK_MODE:
        return {
            "allowed": True,
            "reason": None,
            "suggested_action": None,
            "modified_action": player_action,
            "parsed_intent": {
                "actions": [player_action],
                "dialogues": [],
                "ooc_intent": None
            }
        }
    
    # æ„å»ºæ¶ˆæ¯
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    # å¦‚æœä½¿ç”¨æœ¬åœ° LLMï¼Œæ£€æŸ¥å¹¶æˆªæ–­æ¶ˆæ¯
    if LOCAL_LLM:
        # é¢„ç•™ç©ºé—´ç»™è¾“å‡ºï¼ˆçº¦ 20%ï¼‰
        max_input_tokens = int(MAX_CONTEXT_LENGTH * 0.8)
        messages = truncate_messages_if_needed(messages, max_input_tokens)
    
    # æ„å»ºè¯·æ±‚å‚æ•°
    request_params = {
        "model": os.getenv("OPENAI_MODEL", "gpt-4o"),
        "messages": messages,
        "temperature": 0.3  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§
    }
    # æœ¬åœ° LLM å¯èƒ½ä¸æ”¯æŒ response_formatï¼Œå®Œå…¨ä¸ä¼ é€’è¯¥å‚æ•°
    if not LOCAL_LLM:
        request_params["response_format"] = {"type": "json_object"}
    else:
        # æœ¬åœ° LLM è®¾ç½® max_tokens
        request_params["max_tokens"] = MAX_OUTPUT_TOKENS
    
    response = await client.chat.completions.create(**request_params)
    
    # æ£€æŸ¥å“åº”å®Œæ•´æ€§
    if not response.choices or len(response.choices) == 0:
        raise ValueError("LLM å“åº”ä¸ºç©ºï¼šæ²¡æœ‰è¿”å›ä»»ä½•é€‰æ‹©")
    
    choice = response.choices[0]
    
    # æ£€æŸ¥ finish_reasonï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if hasattr(choice, 'finish_reason') and choice.finish_reason:
        if choice.finish_reason == "length":
            print(f"âš ï¸  è­¦å‘Šï¼šLLM å“åº”å› è¾¾åˆ° max_tokens é™åˆ¶è€Œè¢«æˆªæ–­ (finish_reason: {choice.finish_reason})")
            print(f"   å½“å‰ max_tokens: {request_params.get('max_tokens', 'N/A')}")
        elif choice.finish_reason != "stop":
            print(f"âš ï¸  è­¦å‘Šï¼šLLM å“åº”å¼‚å¸¸ç»“æŸ (finish_reason: {choice.finish_reason})")
    
    content = choice.message.content
    if content is None:
        raise ValueError("LLM å“åº”å†…å®¹ä¸ºç©º")
    
    # è®°å½•å“åº”é•¿åº¦ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    if LOCAL_LLM:
        print(f"ğŸ“ LLM å“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
    
    # æ¸…ç†å’Œä¿®å¤ JSONï¼ˆæœ¬åœ° LLM å¯èƒ½è¿”å›æ ¼å¼ä¸æ­£ç¡®çš„ JSONï¼‰
    if LOCAL_LLM:
        # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆé™¤äº†æ¢è¡Œç¬¦å’Œåˆ¶è¡¨ç¬¦ï¼‰
        content = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f]', '', content)
        # æ›¿æ¢ JSON ç»“æ„ä¸­çš„ä¸­æ–‡æ ‡ç‚¹ç¬¦å·ä¸ºè‹±æ–‡æ ‡ç‚¹
        content = re.sub(r'(")\s*ï¼š\s*', r'\1: ', content)  # ä¸­æ–‡å†’å·
        content = re.sub(r'(")\s*ï¼Œ\s*', r'\1, ', content)  # å­—ç¬¦ä¸²åçš„ä¸­æ–‡é€—å·
        content = re.sub(r'(\})\s*ï¼Œ\s*', r'\1, ', content)  # å¯¹è±¡åçš„ä¸­æ–‡é€—å·
        content = re.sub(r'(\])\s*ï¼Œ\s*', r'\1, ', content)  # æ•°ç»„åçš„ä¸­æ–‡é€—å·
        content = re.sub(r'(\d+|true|false|null)\s*ï¼Œ\s*', r'\1, ', content)  # å€¼åçš„ä¸­æ–‡é€—å·
        
        # ç§»é™¤æœ«å°¾çš„åˆ†éš”çº¿ï¼ˆè°ƒè¯•è¾“å‡ºå¯èƒ½è¢«åŒ…å«åœ¨å“åº”ä¸­ï¼‰
        content = re.sub(r'\s*=+\s*$', '', content, flags=re.MULTILINE)
        
        # å°è¯•æå– JSON å¯¹è±¡ï¼ˆå¦‚æœå“åº”åŒ…å«å…¶ä»–æ–‡æœ¬ï¼‰
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        if json_match:
            content = json_match.group(0)
    
    try:
        return parse_json_with_fallback(content)
    except Exception as e:
        print(f"âš ï¸  JSON è§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ LLM ä¿®å¤: {e}")
        try:
            # å°è¯•ä½¿ç”¨ LLM ä¿®å¤ JSON
            expected_schema = schema_hint if schema_hint else "æ¸¸æˆé€‰é¡¹å“åº”æ ¼å¼ï¼š{\"narrative\": \"...\", \"choices\": [...], \"mood\": \"...\", \"character_positions\": {...}}"
            return await repair_json_with_llm(content, expected_schema=expected_schema)
        except Exception as repair_err:
            print(f"âŒ  LLM ä¿®å¤ä¹Ÿå¤±è´¥: {repair_err}")
            raise e
